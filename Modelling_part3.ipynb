{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPoUnFreSTYa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing important Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Decompress the file\n",
        "import gzip\n",
        "\n",
        "#Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "#text preprocessing\n",
        "from datetime import datetime\n",
        "#text preprocessing\n",
        "import spacy\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "#Modeling\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "#Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "0lilJutRm2nK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading data for modelling \n",
        "df = pd.read_csv('/content/drive/MyDrive/Project/df_r3.csv',index_col=False)\n"
      ],
      "metadata": {
        "id": "2eaSPfKxDDIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check the names of the columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "XGqVynCaH-Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting only useful column for Tableau EDA \n",
        "df_tableau = df[['Rating','reviewerID','style','review_sentiment','DateTime','category','also_buy','brand','feature','also_view']]"
      ],
      "metadata": {
        "id": "bpdaM-NNJHNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check the last 10 rows\n",
        "df_tableau.tail()"
      ],
      "metadata": {
        "id": "QN3-8ImkJXaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save in in csv mode to do EDA in Tableau\n",
        "df_tableau.to_csv('/content/drive/MyDrive/Project/TableauEDA.csv')\n"
      ],
      "metadata": {
        "id": "OAC7L8N7JmRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check first five rows\n",
        "df_tableau.head()"
      ],
      "metadata": {
        "id": "mweUozP9J6VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check the information of the dataset\n",
        "df.info()"
      ],
      "metadata": {
        "id": "QJEz3AfXKIk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Droping unncessary data which is not used for modelling\n",
        "df = df.drop(['Rating','reviewerName','DateTime','description','category','title','also_view','also_buy',\n",
        "              'brand','similar_item','verified','style','vote','feature'],axis=1)"
      ],
      "metadata": {
        "id": "CkvCI0Mb_Fkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check top 5 rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "tX_T_xXP_W2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To make a copy of data\n",
        "df_copy=df.copy()"
      ],
      "metadata": {
        "id": "5yv41OWW_mRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check the values\n",
        "df['rating_class'].values"
      ],
      "metadata": {
        "id": "I9HIm8aPAGgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert label to a numerical variable\n",
        "df['rating_class'] = df.rating_class.map({'bad':0, 'good':1})"
      ],
      "metadata": {
        "id": "2U3njm-MAddB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unncessary columns\n",
        "df = df.drop(['Unnamed: 0','reviewerID','asin','review_sentiment','rank','review_text'],axis=1)"
      ],
      "metadata": {
        "id": "AdTnfSF6As1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking null values\n",
        "df['clean_text'].isnull().sum()"
      ],
      "metadata": {
        "id": "d4I2EDbUB8CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Drop null values from clean_text column\n",
        "df = df.dropna(subset = ['clean_text'])"
      ],
      "metadata": {
        "id": "rhUMaCOGtJhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop(['price'], axis=1)\n",
        "\n",
        "df['clean_text'].isnull().sum()"
      ],
      "metadata": {
        "id": "UmTZMXEStKJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lBb54lNntMIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Rename rating class\n",
        "df.rename(columns={\"rating_class\": \"Positivity\"}, inplace=True)"
      ],
      "metadata": {
        "id": "BMsQ9DR8tOyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "10yarmlMttVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sns.catplot(x=\"Positivity\", data=df, kind=\"count\", height=5, aspect=1.4, palette=\"BrBG\")\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "TVF5aXOGtwGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The bar chat below showing a comparison between positive and negative reviews using phone dataset"
      ],
      "metadata": {
        "id": "hlFcQZkXufkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data into train and test \n",
        "x = df['clean_text']\n",
        "y = df['Positivity']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=0.33,random_state=42)\n",
        "\n",
        "\n",
        "text_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',MultinomialNB())])\n",
        "\n",
        "text_clf.fit(X_train,y_train)\n",
        "\n",
        "predictions = text_clf.predict(X_test)\n",
        "\n",
        "print(confusion_matrix(y_test,predictions))\n",
        "cm = confusion_matrix(y_test,predictions)\n",
        "print(classification_report(y_test,predictions))"
      ],
      "metadata": {
        "id": "picPuTA7uhB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqYoLVnuu6mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This looks nice. We got an accuracy of ~86% on the test set."
      ],
      "metadata": {
        "id": "_M-6zb3zvGul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
        "sns.color_palette(\"husl\", 10)\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix'); \n",
        "ax.xaxis.set_ticklabels(['Negative', 'Positive']); ax.yaxis.set_ticklabels(['Negative', 'Positive']);"
      ],
      "metadata": {
        "id": "C_ptM9KPvIWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**True Positive(TP)**: The prediction outcome is true, and it is true in reality\n",
        "\n",
        "**True Negative(TN)**: The prediction outcome is false, and it is false in reality\n",
        "\n",
        "**False Positive(FP)**: The prediction outcomes are true, but they are false in actuality.\n",
        "\n",
        "**False Negative(FN)**: The predictions are false, and they are true in actuality.\n",
        "\n",
        "**Precision**: It determines the proportion of positive prediction that was actually correct.\n",
        "\n",
        "Precision = TP/TP+FP\n",
        "\n",
        "**Recall/Sensitivity**: It aims to calculate the proportion of actual positive that was identified incorrectly.\n",
        "\n",
        "Recall = TP/TP+FN\n",
        "\n",
        "**Accuracy**: It is defined as the number of correct predictions made as a ratio of all predictions made. The model with the higher accuracy value is considered to be the best model.\n",
        "\n",
        "Accuracy = TP+TN/TP+FP+FN+TN\n",
        "\n",
        "F1 Score: This score will give us the harmonic mean of precision and recall. F1 score is the weighted average of the precision and recall.\n",
        "\n",
        "F1 Score = 2(Recall Precision) / (Recall + Precision)\n",
        "\n"
      ],
      "metadata": {
        "id": "Ps2G9CGZx8x9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjyHFE9x0FsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}